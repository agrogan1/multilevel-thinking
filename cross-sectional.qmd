# The Cross Sectional Multilevel Model

> "Mathematical Science shows us what is. It is the language of unseen relations between things. But to use & apply that language we must be able fully to appreciate, to feel, to seize, the unseen, the unconscious. Imagination too shows us what is, the is that is beyond the senses.” [@Lovelace1992]

> "I’m often asked if there is something I think writers ought to do, and recently in an interview I heard myself say: 'Several things. Love words, agonize over sentences. And pay attention to the world.'" [@Sontag2007]

```{r}

library(haven) # read Stata

library(tibble) # data frames

library(ggplot2) # beautiful graphs

library(geomtextpath) # path geoms

library(dplyr) # data wrangling

library(tidyr) # data tidying

library(pander) # nice tables

# library(gt) # beautiful tables

```

## Introduction

I begin this chapter with some introductory thinking about multilevel modeling, by starting with two key ideas: multilevel models can improve our estimation of p values; multilevel models can improve our estimation of $\beta$ coefficients. \index{cross sectional models}

After introducing these two key concepts of multilevel modeling, I then begin a more in depth exploration of the equations and concepts and statistical syntax of the cross sectional multilevel model.

## Some First Ideas About Multilevel Modeling

### Estimating Standard Errors And p Values {#sec-pvalues}


#### Introducing the Idea

If the data are grouped, nested, or clustered, then this aspect of the structure of the data needs to be accounted for. @Bland1994 describe a simulation in which grouped data are artificially generated according to the following procedure. \index{p values} \index{advantages of the multilevel model}

> "The data were generated from random numbers, and there is no relation between X and Y at all. Firstly, values of X and Y were generated for each 'subject,' then a further random number was added to make the individual observation." [@Bland1994]

```{r}

simulated_clustered_data <-
  read_dta("./simulate-and-analyze-multilevel-data/simulated_clustered_data.dta")

```

The graph below illustrates the process of simulating the data.

```{r}
#| fig-cap: "Simulated Clustered Data"
#| fig-height: 3
#| label: fig-simulatedclustereddata

ggplot(simulated_clustered_data,
       aes(color = factor(group_numA),
           label = group_numA)) +
  geom_segment(aes(x = x_individualA, 
                y = y_individualA,
                xend = x_groupA,
                yend = y_groupA),
               show.legend = FALSE) +
  geom_point(aes(x = x_groupA, 
                 y = y_groupA),
             size = 10,
             alpha = 1,
             show.legend = TRUE) + 
  geom_text(aes(x = x_groupA,
                y = y_groupA),
            color = "white",
            show.legend = FALSE) +
  geom_label(aes(x = x_individualA, 
                y = y_individualA),
             show.legend = FALSE,
             size = 3) + 
  labs(title = "Illustrating the Process of Simulating the Data",
       x = "x",
       y = "y") +
  scale_color_viridis_d(name = "group") +
  theme_minimal() 

```

#### Compare OLS and MLM

An analysis that is not aware of the grouped nature of the data will give biased results, will mis-estimate standard errors, and importantly, will often attribute statistical significance to some of the independent variables when this is not appropriate [@Raudenbush2002; @Bland1994]. \index{p values}

In the example below, we compare a simple ordinary least squares analysis of the data with a multilevel model that accounts for the clustered nature of the data.

```{r, child=c('./simulate-and-analyze-multilevel-data/tableA.md')}
```

We see that in the ordinary least squares analysis, the independent variable is judged to have a statistically significant association with the dependent variable. The more appropriate multilevel model finds that in fact the independent variable $x$ is *not* associated with $y$. Thus, the multilevel model provides more accurate results than OLS in the presence of clustered data.

### Multilevel Structure {#sec-multilevelstructure}

Associations between two variables can be *very different* (or even *reversed*) depending upon whether or not the analysis is "aware" of the grouped, nested, or clustered nature of the data [@Gelman2007]. In the example presented here, the groups are countries, but could as easily be neighborhoods, communities, or schools. 

> For teaching purposes, I use an example with very few clusters, although it would be more appropriate to apply multilevel analysis to an example with many more clusters e.g. ($N_\text{clusters} >= 30$)

A model that is "aware" of the clustered nature of the data may provide very different--likely better--substantive conclusions than a model that is not aware of the clustered nature of the data. \index{sign of coefficients} \index{advantages of the multilevel model}

I use some data simulated for this particular example.

```{r}

multilevelstructure <-
  read_dta("./simulate-and-analyze-multilevel-data/multilevelstructure.dta")

multilevelstructure$country <- factor(multilevelstructure$country)

```

#### Graphs

##### A "Naive" Graph 

This "naive" graph is unaware of the grouped nature of the data. Notice that the overall regression line slopes downward, even though there is some suggestion that *within each group* the regression lines may slope upward.

```{r}
#| fig-cap: "A 'Naive' Graph"
#| fig-height: 3
#| label: fig-naive

p0 <- ggplot(multilevelstructure, 
             aes(x = x,
                 y = y)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "y as a function of x") +
  theme_minimal()

p0  # replay 

```

##### An "Aware" Graph 

This "aware" graph is aware of the grouped nature of the data. The graph is "aware" of the grouped or clustered nature of the data, and provides indication that the regression lines *when accounting for group* slope upward.

```{r}
#| fig-cap: "An 'Aware' Graph"
#| fig-height: 3
#| label: fig-aware

ggplot(multilevelstructure, 
             aes(x = x,
                 y = y,
                 color = country)) + # color is country
  geom_point() + 
  geom_smooth(method = "lm") + 
  scale_color_viridis_d() +
  theme_minimal()

```

#### Regressions: A "Naive" OLS Analysis vs. An "Aware" MLM Analysis

The OLS model with only *x* as a covariate is not aware of the grouped structure of the data, and the coefficient for *x* in the OLS model reflects this. The coefficient for *x* in the OLS model is *negative*, and statistically significant.

The multilevel model is aware of the grouped structure of the data, and the coefficient for *x* in the multilevel model reflects this. The coefficient for *x* in the multilevel model is *positive*, and statistically significant.

```{r, child=c('./simulate-and-analyze-multilevel-data/tableB.md')}
```

#### A Thought Experiment

When might a situation like this arise in practice? This is surprisingly difficult to think through. 

Imagine that *x* is a protective factor, or an intervention or treatment. Imagine that *y* is a desirable outcome, like improved mental health or psychological well being.

Now imagine that residents of countries provide more of the protective factor or more of the intervention in situations where there are lower levels of the desirable outcome. If one thinks about it, this is a very plausible situation. 

> A naive analysis that was unaware of the grouped nature of the data would therefore misconstrue the results, suggesting that the intervention was harmful, when it was in fact helpful.

```{r}
#| fig-cap: "A Heuristic Example"
#| fig-height: 3

p0 + 
  geom_point(aes(color = country)) + # points with country color
  geom_smooth(aes(color = country), # smoothers with country color
              method = "lm") + 
  labs(title = "Desirable Outcome as a Function of Intervention or Treatment",
       x = "intervention or treatment",
       y = "desirable outcome") +
  scale_color_viridis_d()

```

The idea that group level and individual level relationships must be the same has been termed the "ecological fallacy" [@FIREBAUGH20014023; @Hox2018; @Snijders2012]. \index{ecological fallacy}

These data are constructed to provide this kind of extreme example, but it easy to see how multilevel thinking, and multilevel analysis may provide better answers than one would get if one ignored the grouped nature of the data.

## The Equation

The equation for the multilevel model can be written in several ways: as multiple levels of equations; or as a single equation. The advantage of having multiple levels of equations is that these multiple equations make clear the multiple levels of the data, and thus conform to an initial understanding of how a multilevel model should be estimated. However, *results* from multiple levels of equations quickly become difficult to interpret, and thus, I will not spend a great deal of time on discussing empirical results of the two level formulation. Whether multiple levels of equations, or a single equation are employed, the numerical results are equivalent.

### Two Levels of Equations

I start with two levels of equations: Level 1 at the level of the individual; and Level 2 at the level of the country.

#### Level 1 (Individuals)

$$y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \beta_{2j} z_{ij} + e_{ij}$$ {#eq-MLM1}

#### Level 2 (Countries)

$$\beta_{0j} = \gamma_{00} +\gamma_{01} w_j + u_{0j}$$ {#eq-MLM2}

$$\beta_{1j} = \gamma_{10} + u_{1j}$$

$$\beta_{2j} = \gamma_{20}$$

$$\beta_{3j} = \gamma_{30}$$

Here $y_{ij}$ is the dependent variable, or outcome for the model. We note that the $ij$ subscripts indicate that this is outcome $y$ for individual $i$ in country $j$. Note that the outcome is at Level 1, or the level of individuals. $\beta_{0j}$ is a regression intercept, and the other $\beta$'s[^beta:] are regression slope parameters. $x_{ij}$ and $z_{ij}$ are independent variables and $t_{ij}$ is an independent variable indicating the time at which different data points are measured. I note that in this discussion I am *not* considering a model in which there are repeated observations on the same individuals, although the multilevel model is certainly extensible to such cases. $u_{0j}$ is a *random intercept* for the $\beta_{0j}$ term, and $u_{1j}$ is a *random slope* for the $\beta_{1j}$ term, indicating that we are modeling cross country variation in these parameters. The other $\beta$ terms are not modeled as having random country level variation, although this could certainly be a possibility in subsequent models. \index{random intercept} \index{random slope} 

In this formulation of the multilevel model, each regression parameter $\beta$ in the level 1 equation is the outcome of an equation at Level 2. The parameters for the Level 2 equations are represented by $\gamma$'s. $w$ a Level 2 variable appears in the first Level 2 equation.

### One Level of Equations

By simply substituting the values of the Level 2 equations into the Level 1 equations--and rewriting the $\gamma$'s as $\beta$'s--we obtain:

$$y_{ij} = \beta_0 + \beta_1 x_{ij} + \beta_2 z_{ij} + \beta_3 w_{j} + u_{0j} + u_{1j} \times x + e_{ij}$$ {#eq-MLM}

Here again $y_{ij}$ is the dependent variable, or outcome for the model. $\beta_0$ is a regression intercept, and the $\beta$'s are regression parameters. $x_{ij}$ and $z_{ij}$ are independent variables and $w$ is a Level 2 variable. 

> Notice that in this *single equation* format all variables--no matter their *level*--appear in the same equation. 

In this formulation of the equation, the nature of the random effects is more clear, and merits discussion. Notice that we have included a *random intercept*, $u_{0j}$ as well as a *random slope*, $u_{1j} \times x$. The *random intercept*, $u_{0j}$, indicates that there is variation in the *intercept* of the country specific regression lines, as is true in @fig-data. The *random slope* term associated with $x$, $u_{1j} \times x$, indicates that we are allowing for the possibility of variation in the *slope* of the regression lines that is associated with $x$, in this case, the slope of parental warmth, as is possibly suggested in @fig-data. 

[^beta:]: Technically, all of these $\beta$'s could be written as $\beta_j$ since the multilevel model could be said to estimate a regression parameter for each group, in this case each country. One could even write $\beta_{jk}$ to represent the regression parameter for the $k^{th}$ independent variable the for the $j^{th}$ group or country. To keep matters simple, I simply write $\beta$ in most cases.

To make these ideas more concrete, I rewrite this equation in terms of the main substantive ideas of this book:

$$\text{outcome}_{ij} = \beta_0 + \beta_1 \text{parental warmth}_{ij} + \\ \beta_2 \text{physical punishment}_{ij} +$$ {#eq-MLMsubstantive}

$$\beta_3 \text{identity}_{ij} + \beta_4 \text{intervention}_{ij} + \beta_5 \text{HDI}_{ij} + $$
$$u_{0j} + u_{1j} \times \text{parental warmth} + e_{ij}$$ 

Put substantively, this model indicates that the outcome can be conceptualized as a function of an intercept term, and contributions of parental warmth, physical punishment, identity group membership, participation in the intervention, and country level HDI. The *random intercept*, $u_{0j}$ indicates that there is some unexplained variation in the outcome at the country level. The *random slope*, $u_{1j} \times \text{parental warmth}$ indicates that the model is allowing for country level variation in the association of parental warmth with the outcome. Inspection of @fig-data indicates that it might be possible that there would be variation across countries in this slope. The model could be extended to allow for country level variation in other slope terms by adding other random slopes, eg $u_{2j}$, $u_{3j}$, etc.

## Regression With Simulated Multi-Country Data {#sec-regression}

After considering some of these broader issues, let's now examine the results of a multilevel regression with the simulated multicountry data. I will again imagine that the desirable outcome is an outcome such as improved psychological wellbeing. 

### Unconditional Model {#sec-unconditional}

The unconditional model is a model with no $x$'s or covariates [@Raudenbush2002]. \index{unconditional model}

$$\text{outcome}_{ij} = \beta_0 + u_{0j} + e_{ij}$$ {#eq-unconditional}

Here, $\text{outcome}_{ij}$ is a function of an intercept $\beta_0$, a country specific error term, $u_{0j}$, and an individual level error term $e_{ij}$.

Thus, all of the variation in $\text{outcome}_{ij}$ is--given the *unconditional* nature of our model--attributable to unmeasured variation at the country and individual level.

### Intra-Class Correlation Coefficient {#sec-ICC}

I now introduce a measure known as the Intra-Class Correlation Coefficient, (ICC) that can be computed from this unconditional model [@Raudenbush2002]. \index{variation and diversity} \index{ICC}

$$\text{ICC} = \frac{var(u_{0j})}{var(u_{0j}) + var(e_{ij})}$$ {#eq-ICC}

Heuristically:

$$\text{ICC} = \frac{\text{group level variation}}{\text{group level variation} + \text{individual level variation}} = $$ {#eq-ICCheuristic}

$$\frac{\text{group level variation}}{\text{total variation}}$$

The ICC from the *unconditional* model (@eq-unconditional) is the most informative ICC as it represents the amount of variation in the dependent variable that could *potentially* be explained by the grouping variable.

Put another way, in a two level model, the ICC provides a quantitative measure of the amount of variation in a measure that is present at Level 2. Knowing the ICC, we can then easily calculate the percentage of variation at Level 1: 

$$\text{Level 1 variation} = \text{total variation} - \text{Level 2 variation} =$$
$$= 100\% - (100\% \times \text{ICC})$$

Thus, in some broader sense, the ICC might be thought of as a measure of diversity: the higher the ICC, the higher the clustering of the data in groups, and more of the diversity in the sample is between countries; the lower the ICC, the lower the clustering of the data, and less of the diversity in the sample is between countries.

In practice, I have often found the ICC in cross-sectional models to be relatively low: often on the order of 5%, 10% or 15%. Phrased in terms of diversity and variation, this means that in cross-sectional applications, there is often much more diversity and variation within each group indicated by the grouping variable (e.g. neighborhood, school, country) than there is between the values of the  grouping variable.

```{r, child=c('./simulate-and-analyze-multilevel-data/table0.md')}
```

From using procedures to estimate the ICC, as detailed in the Appendix, or calculating by hand, we see that the ICC for this data is .076 or 7.6%.

As we add covariates, $x$'s, to the model the ICC will most often decrease.

### Conditional Model {#sec-conditional-model}

We next estimate a *conditional* model, *with* independent variables.

```{r, child=c('./simulate-and-analyze-multilevel-data/table1.md')}
```

The data suggest that parental warmth is positively associated with the desirable outcome, and that this result is statistically significant. Parental use of physical punishment is associated with statistically significant decreases in the desirable outcome. The identity variable is not associated here with the outcome. In contrast, the application of the intervention is associated with increases in the outcome. 

I note that there is some variation in the *constant* indicating that there is some variation in the initial or average levels of the desirable outcome--again improved psychological well-being--that is attributable to country.

There is--in contrast--no discernible variation in the *slope* associated with parental warmth that is attributable to country. Thus, the relationship of parental warmth with child outcomes does not appear to differ appreciably from country to country. Had there been statistically significant variation in this *random slope*, it would have indicated that the association of parental warmth with the outcome varied across countries.

`HDI`, the *Human Development Index*, our only country level, or Level 2, variable in this model is not associated with the outcome.

::: {.callout-tip}
### Intuitions

*Random intercepts* and *random slopes* are statistically closely related concepts. In my experience of over 10 years of teaching multilevel modeling, I have found that *random intercepts* are more intuitive, while a fuller set of intuitions about what *random slopes* substantively imply seems to be much more difficult, and to take much more time to fully understand.

Briefly, *random intercepts* refer to group level (e.g. neighborhood level, school level, or country level) variation or diversity in the initial level—or equivalently the overall level—of some outcome like behavior or mental health. Put concretely: do, for example, neighborhoods, or schools, or countries, systematically differ in the *overall level of some outcome* like behavior or mental health?

In contrast, *random slopes* refer to group level (e.g. neighborhood level, school level, or country level) variation or diversity in *the relationship of a predictor to an outcome*. Put concretely, for example: 

1. Does the *relationship of parenting with an outcome* differ systematically by neighborhoods, or schools, or countries? 
2. Does the *relationship of some identity with an outcome* differ systematically by neighborhoods, or schools, or countries?
3. Does the *relationship of an intervention with an outcome* differ systematically by neighborhoods, or schools, or countries? 
:::

## Indicator Variables, Random Intercepts and Random Slopes, and Identities

### Variable Types

In thinking about any statistical analysis, it is important to think about different types of variables. Thinking about different types of variables may seem to be an arcane or obscure topic. Statisticians can make very fine distinctions between different types of variables, and it sometimes seems that there are as many typologies of variables as there are statisticians. Nonetheless some broad distinctions are useful. \index{types of variables}

In a classic text, @Freedman1991 offer a short definition of variable types that provides some useful crucial distinctions:

> "Some questions are answered by giving a number: the corresponding variables are *quantitative*. Age, family size, and family income are examples of quantitative variables. Some questions are answered with adjectives, and the corresponding variables are *qualitative*: examples are marital status (single, married, widowed, divorced, separated) and employment status (employed, unemployed, not in the labor force)." 

When teaching and learning about statistical software, I find it easiest to re-label *quantitative* variables as *continuous* or *numeric* variables, and *qualitative* variables as *categorical* variables.[^finer_distinctions]

[^finer_distinctions]: I recognize that there are finer distinctions to be made here, many of which @Freedman1991--as well as other introductory texts--go on to make. However, for the purposes of explication in this book, I find the *continuous/numeric* versus *categorical* variable distinction to be the most useful and the one that most readily facilitates communication with faculty colleagues and with students. 

### Categorical Variables as Indicators or Random Intercepts

Statistically, there is often a concern about whether a particular qualitative or categorical variable is most appropriately modeled as an indicator variable, or as a level (random intercept). Substantively, this question intersects with how one should statistically include and model various measures of identity. Many identities are often measured as categorical variables. \index{indicator variables} \index{random intercept}

### The Example of U.S. Census Data

The United States Census [@Census2022] asks about race using several different categories: White; Black or African American; American Indian or Alaska Native; Asian; Native Hawaiian and Pacific Islander; Some Other Race. Followup questions are asked for some identities, and a separate question is asked about Latino / Hispanic identity. Thus one might imagine a questionnaire with two measures. \index{Census}

| Race                                 | Latino                 |
|--------------------------------------|------------------------|
| White                                | Latino or Hispanic     |
| Black or African American            | Not Latino or Hispanic |
| American Indian or Alaska Native     |                        |
| Asian                                |                        |
| Native Hawaiian and Pacific Islander |                        |
| Some Other Race                      |                        |

: Hypothetical Questions Measuring Race {#tbl-race-ethnicity-questions}

A table of data might then look something like the following.

```{r}
#| label: tbl-race-ethnicity-data
#| tbl-cap: Simulated Data on Race and Ethnicity
#| echo: false

race_ethnicity <- read_dta("simulate-and-analyze-multilevel-data/race_ethnicity.dta")

pander(head(race_ethnicity))

```

More recently, the Census has allowed respondents to select multiple racial identities. Such a table of data might look like this.

```{r}
#| label: tbl-race-ethnicity-2-data
#| tbl-cap: Simulated Data on Race and Ethnicity With Multiple Identities
#| echo: false

race_ethnicity2 <- read_dta("simulate-and-analyze-multilevel-data/race_ethnicity2.dta")

pander(head(race_ethnicity2))

```

### Identities As Indicator Variables

Identities can be modeled as indicator variables. Importantly, whether only a single identity is selected (e.g. @tbl-race-ethnicity-data), or multiple identities are selected (@tbl-race-ethnicity-2-data), if identities are modeled as indicator variables, they should be modeled as a *set* of indicator variables with an omitted reference category. If there are $k$ categories of a variable, there should be $k-1$  indicator variables., e.g.

$$y_{ij} = \beta_0 + \beta_2 \text{race2} + \beta_3 \text{race3} + \beta_4 \text{race4} + \beta_5 \text{race5} + \beta_6 \text{race6} + $$  {#eq-indicator-variables} 

$$\beta_7 \text{latinx} + u_{0j} + e_{ij}$$ 

In @eq-indicator-variables the first group of race, `race1` is treated as the reference category, and is therefore omitted from the model, and all comparisons are made to individuals with identity `race1`.

::: {.callout-tip}
#### Identities As Indicator Variables in Equations And Syntax

Thus, if an identity, such as race, is used as an *indicator variable*, it would appear in the equations (e.g. @eq-MLMsubstantive), and in the statistical syntax covered in the Appendix, in the same place as the `identity` variable. 
:::

::: {.callout-tip}
#### Reference Categories Need To Be Chosen Carefully

Reference categories need to be chosen carefully. This is especially true in studies that hope to be attentive to diversity and social change. The default in many software programs is to exclude the first category of an identity, as is evident in @eq-indicator-variables. However, in the example given in @tbl-race-ethnicity-questions, this would mean that all comparisons would be made to individuals who identified as *white*. It often makes sense to choose a different reference category than the default, and investigators are encouraged to be thoughtful about the choice of reference category for indicator variables. In most software, the choice of reference categories can be manually specified.
:::

::: {.callout-warning}
#### Indicator Variables Should Not Be Treated As Continuous Variables

Identities that are measured in the data with a single categorical variable with values like `1`, `2`, `3`, `4`, `5`, `6` should *not* be modeled as a single continuous variable. For example, if the data are similar to those presented in @tbl-race-ethnicity-data--where identity is measured as a single column of data--it is important that the multiple identities *not* be modeled as a single variable, which would end up *de facto* treating this multi-category *categorical* variable as a single *continuous* variable. Instead, multiple indicator variables ($k-1$) for the multiple ($k$) identities should be clearly specified in one's equation (as in @eq-indicator-variables) and in the statistical syntax. Details of how to accomplish this for different software packages are included in the Appendix.
:::

### Identities as Random Intercepts or Slopes

If there are many values of an identity it may be difficult to model those multiple identities as indicator variables. For example, if there were 10 possible values of a particular identity, and one was modeling those identities with indicator variables, one would need to include 9 ($k-1$) indicator variables. Those 9 indicator variables could potentially use a large fraction of the available degrees of freedom, and interpreting the $\beta$ regression coefficients for 9 indicator variables might also be challenging. The problem is even greater when one considers a larger number of identities (e.g. 25, 30, 50, 100), as is possible in some data sets. In a global or cross cultural data set, such as the simulated data that is used as an example throughout this book, one might easily conceive of data with 20, 30 or 50 different identities, especially since racial and ethnic identity categories may differ substantially across countries, around the world [@Rocha2020].

An alternative would be to model large numbers of possible identities as a random intercept, rather than as a set of indicator variables. Such an equation might appear as follows:

$$y_{ij} = \beta_0 + \Sigma \beta_m \text{covariates} + v_{0k} + u_{0j} + e_{ij}$$ {#eq-identity-random-intercept}

Here, the inclusion of $v_{0k}$, a random intercept for identity, would be a way to model the presence of many possible values of an identity variable. 

::: {.callout-tip}
#### Identities As Random Intercepts in Equations And Syntax

Thus, if an identity, such as race, is used as a *random intercept*, it would appear in the equations (e.g. @eq-MLMsubstantive), and in the statistical syntax covered in the Appendix, in the same place as the `country` variable. 
:::

An *advantage* of the approach outlined in @eq-identity-random-intercept is that we could model multiple identities without including a large--potentially enormous--number of indicator variables. We would thus avoid using up a large number of degrees of freedom in our data set. We would also avoid the conceptual difficulties that might come from including a large number of indicator variables in our data set. For example, if we had 30 categories of a particular categorical variable, it might be difficult to interpret the resultant 29 regression coefficients for the 29 indicator variables. At the same time, it would be straightforward to include a random intercept that had 30, or even 50, or 100, or 1,000 possible values.

A potential *disadvantage* of the approach outlined in @eq-identity-random-intercept is that this is multi-country data that already employs a random slope, $u_{0j}$. Adding $v_{0k}$ means that we now have a model with *two* random slopes, a topic which is discussed in **Chapter XXXXXXX**.

A *caveat* of the approach outlined in @eq-identity-random-intercept is that random intercepts (e.g. $v_{0k}$ and $u_{0j}$)--which are essentially error terms at level 2--are assumed to follow a normal distribution [@StataCorp2021:2]. Ten identities, to follow the example above, is generally considered to be too few groups to satisfy this assumption of a normal distribution. @Maas2004 suggest that at least 50 level 2 units are necessary to ensure satisfaction of this normality assumption. Importantly, if one has fewer than 50 units, the bias introduced only appears to affect the estimation of the level 2 random intercepts and slopes, not the estimation of the independent variables [@Maas2004; @Maas2005]. If one has fewer than this number one possible procedure is to model the groups both as a *random intercept* and as *a set of indicator variables*, and to see if the different approaches lead to different substantive conclusions [@Gershoff2010]. 

A second *caveat* of modeling identities as a random intercept is that these identities would need to be mutually exclusive. For example, "black", "white" and "identifies as both black and white", would all need to be separately coded identities under this approach.

## Correlation of Random Intercept and Random Slope(s)

To further elaborate the cross-sectional multilevel model that we have been considering, we could also consider a situation in which a random slope or slopes were *correlated* with each other, and with the random intercept. In the equation that we are considering, this would entail estimation of whether or not, the random intercept, $u_{0j}$, was correlated with the random slope for warmth, $u_{1j}$. \index{correlation of random effects}

Substantively, this question would be asking whether the association of warmth and the outcome, was correlated with the initial level or average level of the outcome. From @fig-data, it appears that there is some slight evidence that the country specific regression slopes are more steep in countries where the initial level of the outcome is higher. However, we may wish to investigate this question more rigorously.

Procedures for estimating models with correlated or uncorrelated random effects vary across software. I illustrate this issue in @eq-varcovar below, where the diagonal elements are the *variances* of each of the random effects, and the off diagonals, which would be the *covariances* of the random effects are constrained to 0.

$$\begin{bmatrix}
var(u_{0j}) & 0 \\
0 & var(u_{1j}) 
\end{bmatrix}$$ {#eq-varcovar}

In contrast, we might wish to estimate a model in which the random effects are allowed to be correlated.

$$\begin{bmatrix}
var(u_{0j}) & cov(u_{0j}, u_{1j}) \\
cov(u_{0j}, u_{1j}) & var(u_{1j}) 
\end{bmatrix}$$ {#eq-varcovaruns}

When we estimate such a model, we get the following information.

```{r, child=c('./simulate-and-analyze-multilevel-data/table1A.md')}
```

Results are mostly similar to those above. However, here, we are asking additionally for information about the possible *correlation* of country specific initial levels of the outcome and the slope of the country specific regression line for parental warmth. Results indicate that there is no reason to be believe that these two parameters are correlated. Put more intuitively, it does not appear that parental warmth is any more or less correlated with the outcome in countries where initial levels of the outcome are higher. Again, had this correlation been statistically significant and positive, it would have indicated that higher initial, or average levels of the outcome were associated with a greater association of warmth with the outcome.

## Within and Between {#sec-withinbetween}

```{r}

simulated_multilevel_data <- read_dta("simulate-and-analyze-multilevel-data/simulated_multilevel_data.dta")

```

Coefficients in models can be divided into within and between. A substantive example may be helpful here. When we consider the variable of parental `warmth`, we can imagine the parental warmth expressed in each family, $\text{warmth}_{ij}$, representing family *i* in country *j*. We can also think about the *grand mean* of warmth across the entire sample, $\overline{\text{warmth}}_{..}$. We can then also think about the mean expression of parental warmth in each country, $\overline{\text{warmth}}_{.j}$, i.e. the mean level of parental warmth in country *j*. \index{within and between}

```{r}
#| fig-cap: "Distribution of Parental Warmth Across Countries"
#| fig-height: 6
#| label: fig-distributionwarmth

library(ggplot2)

simulated_multilevel_data$country <- factor(simulated_multilevel_data$country)

ggplot(simulated_multilevel_data,
       aes(x = warmth,
           fill = country)) +
  geom_bar() +
  scale_fill_viridis_d(name = "Country") +
  facet_wrap(~country) +
  labs(title = "Levels of Parental Warmth",
       subtitle = "by Country",
       x = "Parental Warmth",
       caption = "Every country has a different distribution of parental warmth \nand mean level of parental warmth") +
  # coord_flip() +
  theme_minimal() +
  theme(axis.text = element_text(size = rel(.5)))

```

Bearing this in mind, one can then think about the *difference* between each individual expression of parental warmth and the overall, or grand mean: $\text{warmth}_{ij} - \overline{\text{warmth}}_{..}$. This value can then be decomposed into two values:

$$\text{warmth}_{ij} - \overline{\text{warmth}}_{..} = \text{warmth}_{ij} - \overline{\text{warmth}}_{.j} + \overline{\text{warmth}}_{.j} - \overline{\text{warmth}}_{..}$$
Put into words, this equation says that the difference in parental warmth displayed by family i in country j from the overall or grand mean of parental warmth is composed of two components:

* *Within Country Component*: How is the level of warmth expressed by family *i* in country *j* different from the *mean* level of warmth in country *j*. Is family *i* different from the *average* family in country *j*? For this particular country, is this a family that is higher, or lower, than average in parental warmth?
* *Between Country Component*: How is the *mean* level of warmth in country *j* different from the overall or *grand mean* level of warmth in the sample as a whole? To what degree is country *j* different from *all countries* in the sample? Is this country a country where parents tend to be higher, or lower, in parental warmth?

Theoretically, or conceptually, one might imagine that it would be useful to decompose a particular behavior into within country and between country components. The within country component could be theorized as *how an individual family differs from their context*, and the between country component could be theorized as *how a particular context differs from the average context*. \index{variation and diversity}

```{r}
#| fig-cap: "Decomposing a Variable into Within and Between Differences"
#| fig-height: 3
#| label: fig-withinbetween

aggdata <- multilevelstructure %>%
  filter(country != 2) %>%
  group_by(country) %>%
  summarise(mean_x = mean(x))

multilevelstructure %>%
  filter(country != 2) %>%
ggplot(aes(x = x,
           fill = factor(country))) +
  geom_density(alpha = .5) +
  geom_vline(aes(xintercept = mean(x)), 
             linewidth = 2) +
  geom_vline(data = aggdata, 
             aes(xintercept = mean_x)) +
  geom_segment(aes(x = 15.5, 
                   xend = 25.5,
                   y = .05, 
                   yend = .05),
               arrow = arrow(ends = "both",
                             length = unit(0.1, 
                                           "inches"))) +
  geom_segment(aes(x = 25.5,
                   xend = 21,
                   y = .025,
                   yend = .025),
               arrow = arrow(ends = "both",
                             length = unit(0.1, 
                                           "inches"))) +
  geom_point(aes(x = 21, 
                 y = .025),
             color = "red",
             show.legend = FALSE) +
  annotate("text", 
           x = 19, 
           y = .07, 
           label = "\ncountry mean - \ngrand mean",
           size = 3) +
  annotate("text", 
           x = 22, 
           y = .015, 
           label = "\nindividual mean - \ncountry mean",
           size = 3) +
  scale_fill_manual(name = "Country",
                    values = c("#440154", "#FDE725"),
                    labels = c("A", "B")) +
  labs(title = "Components of An Individual Score",
       y = "") +
  theme_minimal()

```

In terms of using statistical software, we need to follow a few steps.

1. Calculate the *grand mean* of the variable.

2. Calculate *country specific means* of the variable.

3. Calculate:
    + individual scores - country specific means
    + country specific means - grand mean
    
4. Estimate the model with within and between.

```{r, child=c('./simulate-and-analyze-multilevel-data/table1B.md')}
```

Estimates suggest that both the difference in an individual family's expression of parental warmth from the country level mean, *but not* the difference in the country level mean from the grand mean are statistically significant predictors of the outcome. 

```{comment, eval=FALSE, echo=FALSE}

## Predicted Values

**To Be Developed:** According to **"Stein's Paradox"**, predictions from a multilevel model may be better than the mean (**shrinkage**).

```

## Summary of Advantages Of The Multilevel Model

The discussion so far gives an idea of the advantages of the multilevel model for studying intrinsically multilevel data: children in classrooms or schools; individuals or families in neighborhoods; individuals or families in countries. These advantages can be summarized below: \index{advantages of the multilevel model}

1. Standard errors are estimated correctly as is statistical significance. This means that p values are correctly estimated accounting for the clustered or nested nature of the data. More colloquially, this most often means that we do not make the mistake of attributing statistical significance to a given risk or protective factor, when such a statistical significance is not warranted. Put even more straightforwardly correct estimation of standard errors and statistical significance prevents us from seeing results that are simply not present in the data, whether those concern risk factors or protective factors. \index{p values}
2. Regression coefficients are estimated correctly accounting for the clustered or nested structure of the data. If one does not account for the clustered or nested structure of the data, regression slopes can be estimated as negative when they are more correctly estimated as positive, or as null, or conversely estimated as positive when there are more correctly seen as negative (or null). Again, to phrase things in a more colloquial fashion, this means that we do not judge something to be a risk factor when it is in fact a protective factor or a null effect; or a protective factor when it is in fact a risk factor, or a null effect. \index{sign of coefficients}

```{comment, eval=FALSE, echo=FALSE}

3. An increasing focus of statistical estimation is not to focus on particular regression parameters, but instead to predict outcomes for particular combinations of independent variables. Predictions from a multilevel model could be said to be best predictions in that groups are weighted by their precision, contributing to an estimate which makes better predictions than would a simple average. More colloquially, multilevel models allow us to predict outcomes better and more accurately than would be possible with simple or more naïve models.

```

## Some Wrong (or Partially Wrong) Approaches {#sec-wrongapproaches}

When data are clustered--e.g. residents in neighborhoods, children in schools, families in countries--it is worth discussing the fact that we have several choices statistically as how to proceed, other than using a multilevel model. Given the discussion so far, we can see the advantages of a multilevel model over these other approaches: 

1. First, we could simply ignore the clustering, and treat the data as though it were composed of statistically independent individuals, i.e. statistically independent $e_i$. As we have discussed above, however, this approach has at least two disadvantages. First, as discussed in @sec-pvalues, this approach will mis-estimate standard errors, most often underestimating them, resulting in underestimated p values and false positives. Second, as discussed in @sec-multilevelstructure ignoring clustering runs the risk of estimating regression $\beta$'s that are not estimated with information about the multilevel structure of the data, with the possibility that $\beta$ coefficients may not only have incorrect statistical significance, but also incorrect magnitude, and even incorrect sign. \index{p values} \index{sign of coefficients}
2. A second approach would be to *aggregate* the data to the level of the higher social unit, e.g. aggregating the data at the level of the neighborhood. Here we run into an idea similar to that discussed in @sec-multilevelstructure, the "ecological fallacy": the idea that group level and individual level relationships are necessarily the same [@FIREBAUGH20014023]. \index{ecological fallacy}
3. Lastly, we could adopt a statistical strategy of *clustering* the standard errors. Clustering the standard errors means that standard errors are corrected for the non-independence of the $e_i$ within clusters. Thus, *p* values are estimated correctly. However, clustering still does not account for the multilevel structure of the data (@sec-multilevelstructure), and thus when relationships between *x*'s and *y* at different levels of the data are very different, simply clustering the standard errors may not give correct estimates of the $\beta$'s. \index{p values}

## Variation {#sec-studyvariation2}

Above, in @sec-studyvariation, I have referred to multilevel models as the study and exploration of variation. Now that I have provided some discussion of the multilevel model, more statistical "unpacking" of ideas about variation is warranted. \index{variation and diversity}

I provide again, for pedagogical purposes, the example substantive equation (@eq-MLMsubstantive) that I have been using in this book.

$$\text{outcome}_{ij} = \beta_0 + \beta_1 \text{parental warmth}_{ij} + \\ \beta_2 \text{physical punishment}_{ij} +$$
$$\beta_3 \text{identity}_{ij} + \beta_4 \text{intervention}_{ij} + \beta_5 \text{HDI}_{ij} + $$
$$u_{0j} + u_{1j} \times \text{parental warmth} + e_{ij}$$ {#eq-MLMsubstantive2}

### Measured and Unmeasured Variation {#sec-measured-unmeasured-variation}

An equation for a multilevel model can be divided into measured and unmeasured variation. Below I use a simplified form of @eq-MLMsubstantive2, focusing in particular--for sake of illustration--on the identity variable.

$$\underbrace{y_{ij}}_\text{outcome} = \underbrace{\beta_0}_\text{intercept} + \underbrace{\beta_1 x}_{\text{slope of} \\ \text{measured x}} + \underbrace{\beta_2 \text{identity}}_{\substack{\text{association of} \\ \text{measured identity} \\ \text{with intercept}}} + \underbrace{\beta_3 x \times \text{identity}}_{\substack{\text{association of} \\ \text{measured identity} \\ \text{with slope of} \\  \text{measured x}}} + $$ 

$$\underbrace{u_{0j}}_{\substack{\text{unmeasured} \\ \text{Level 2} \\ \text{variation} \\ \text{in intercept}}} + \underbrace{u_{1j} \times x}_{\substack{\text{unmeasured} \\ \text{Level 2} \\ \text{variation} \\ \text{in slope of x}}} + \underbrace{e_{ij}}_{\substack{\text{unmeasured} \\ \text{individual} \\ \text{error}}}$$

I have already introduced the idea of an unconditional model (@sec-unconditional), in which there are no independent variables, and all of the variation is unmeasured. The unconditional intraclcass correlation coefficient (ICC) (@sec-ICC) is a measure of the amount of variation that could potentially be attributable to the Level 2 units, in this case, different countries.

```{r}
#| label: fig-variationsources
#| fig-cap: "Sources of Variation in a Multilevel Model"
#| fig-height: 6

tribble(
  ~beta0, ~beta1, ~u0, ~u1,
  5, 0, .5, .25) %>% 
  uncount(weights = 30) %>%
  mutate(country = factor(row_number())) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(intercept = beta0 + rnorm(1, 0, u0),
         slope = beta1 + rnorm(1, 0, u1)) %>%
  ggplot() +
  geom_abline(aes(intercept = intercept, 
                  slope = slope),
              color = "grey") +
  geom_segment(aes(x = 0, 
                   xend = 0,
                   y = max(intercept),
                   yend = min(intercept)),
               alpha = .25,
               color = "#440154",
               size = 1,
               arrow = arrow(ends = "both",
                             length = unit(0.1, 
                                           "inches"))) +
  # annotate(geom = "point", 
  #          x = 0, 
  #          y = 5, 
  #          color = "#FDE725",
  #          alpha = .5,
  #          size = 25) +
  annotate("text", 
           x = 2.0, 
           y = 2.5, 
           label = "variation in intercept",
           color = "#440154") +
  annotate("segment", 
           x = 2, 
           xend = .1, 
           y = 3, 
           yend = 5,
           colour = "#440154", 
           linewidth = 1, 
           arrow = arrow()) +
  annotate("segment", 
           x = 0, 
           xend = 10, 
           y = 1, 
           yend = 1,
           colour = "#440154", 
           linewidth = 1, 
           arrow = arrow(ends = "both")) +
  annotate("text",
           x = 5,
           y = .5,
           label = "variation in x",
           color = "#440154") +
  geom_labelabline(aes(intercept = 5, 
                       slope = max(slope), 
                       label = "variation in slope"),
                   size = 2,
                   text_only = TRUE,
                   textcolor = "#440154",
                   linecolor = "#440154") +
  geom_labelabline(aes(intercept = 5, 
                       slope = min(slope), 
                       label = "variation in slope"),
                   size = 2,
                   text_only = TRUE,
                   textcolor = "#440154",
                   linecolor = "#440154") +
  labs(title = "Sources of Variation",
       subtitle = "In A Multilevel Model",
       x = "x",
       y = "y") +
  xlim(0, 10) +
  ylim(0, 10) +
  scale_color_viridis_d() +
  theme_minimal() 

```
 
### Variation In Intercepts or Outcomes

In @eq-MLMsubstantive2, $var(u_{0j})$ is the model estimated amount of variation in the *outcome*, $y_{ij}$. \index{variation in intercepts or outcomes}

In the regression in @sec-regression, there is discernible between country variation, but more of the variation is between individuals within the same country. Put another way, there is a moderate tendency for children in families in the same country to have similar outcomes, but two children in families in the same country may also have very different outcomes. Children from families in different countries may be as similar as children from families in the same country.

### Variation In Predictors

Equally important, I think, but much less frequently explored than variation in *outcomes*, is the possibility of variation in *predictors*, $var(x_{ij})$. In the substantive example that we have employed so far, the *predictors* are different *parenting behaviors*, so considering variation in *predictors* allows us to consider variation in *parenting behaviors*, as well as variation in the *outcomes* of those behaviors. \index{variation in predictors}

We would estimate variation in behaviors attributable to country in much the same way that we would estimate variation in outcomes, estimating an unconditional model, but substituting $x$ for $y$.[^w]

$$x_{ij} = \beta_0 + w_{0j} + e_{ij}$$ {#eq-unconditionalx}

Then, similarly, the variation in a predictor attributable to the clustered nature of the data--in this case the clustering of individuals in countries--is given by:

$$\text{ICC}_x = \frac{var(w_{0j})}{var(w_{0j}) + var(e_{ij})}$$ {#eq-ICCx} 

[^w]: Here for the sake of clarity, I use $w_{0j}$ as a random effect to think about country specific variation in $x$.

### Variation in Slopes

Another possible type of variation to investigate is variation in the relationship of $x$ and $y$, which is represented in the multilevel model by examining variation in the $\beta$'s, i.e. $var(u_{1j})$. \index{variation in slopes}

### Summary

Thus, we can consider a number of sources of possible variation.

| Model Parameter       | Meaning         |   
|-----------------------|-----------------|
| Independent Variables |                 |
| $var(x_{ij})$         | What is the variation in x? |
| $range(x_{ij})$       | What are the maximum and minimum of x? |
| $var(w_{0j}) \text{ if } x = \beta_0 + w_{0j} + e_{ij}$ | What is the country specific variation in the value of x? | 
| Dependent Variable    | 
| $var(y_{ij})$         | What is the variation in y? |
| $range(y_{ij})$       | What are the maximum and minimum of y? |
| $var(u_{0j})$              | What is the country specific variation in the intercept of y? |   
| Regression Coefficients for Slopes |     |
| $\beta_{x} x$ | What is the relationship of x and y? |
| $\beta_{xz} z \times x$ | What is the effect of z on the relationship of x and y? |
| $var(u_{1j}) \text{ from } u_{1j} \times x$  | What is the country specific variation in the relationship of x and y? |     
| $cov(u_{0j}, u_{1j})$ | What is the covariance of the country specific intercept and and country specific slope. Is the country specific intercept related to the country specific slope? |   

:  Some Possible Sources of Variation To Consider in A Multilevel Model {#tbl-sourcesvariation}

### Variation As An Outcome

Even less common is to examine *variation* itself as an outcome [@Burkner2018]. \index{variation as an outcome}

$$\sigma_{yij} = \beta_0 + \beta_1 x_1 + u_{0j} + e_{ij}$$ {#eq-distributional}

Here, the variation in the outcome, $\sigma_{yij}$, rather than the mean level of the outcome, $y_{ij}$, is the focus of interest. My notation for @eq-distributional draws upon @Burkner2018's notation, but is modified in order to be consistent with the rest of this book. 

Why might such models be of conceptual interest? Imagine for example, that the *variation* in psychological well-being is higher in countries with higher levels of poverty, or higher levels of income inequality. The use of such models as this, discussed in more detail by @Burkner2018, would allow us to explore such a question. 

Of note, while I do not explore in detail differences between Bayesian and frequentist approaches to multilevel modeling in this book, these models are likely to be only estimable with Bayesian software rather than with frequentist software [@Burkner2018]. 

### Maximal Models

Hypothetically, one might imagine that there could be group level unobserved factors which affect regression slopes:  i.e. the relationship between a predictor x and outcome variable y. Arguably, were one to ignore these unobserved factors in statistical estimation, they would show up either in an error term, or in the regression coefficients themselves. Were they to show up in the regression coefficients this would represent statistical bias and a substantive mis-estimation of important effects. thus, there is a conceptual argument for including as many random effects—i.e. random slopes—in a statistical model as possible. \index{maximal models}

Models with all possible random effects are termed *maximal models* [@Barr2013; @Frank2018]. Such models include a large number of random slopes, e.g. $u_1 \times x_1, u_2 \times x_2, u_3 \times x_3, ..., \text{etc.}$ even when some of those estimated slopes are close to 0. Such models may be more easily estimable when using Bayesian estimation [@Frank2018], a topic which I do not cover in detail in this book. 

It should be noted that @Matuschek2017 argue that such a *maximal* approach may lead to a loss of statistical power and further argue that one should adhere to "a random effect structure that is supported by the data." In contrast, @nalborczyk_batailler_loevenbruck_vilain_burkner_2019 argue that maximal models are supported under the Bayesian approach. @Oberauer2022 also argues for including multiple random slopes. @Schielzeth2009 make a similar argument from a frequentist perspective.

