# The Cross Sectional Multilevel Model

```{r}

library(haven) # read Stata

library(tibble) # data frames

library(ggplot2) # beautiful graphs

library(geomtextpath) # path geoms

library(dplyr) # data wrangling

library(tidyr) # data tidying

library(pander) # nice tables

# library(gt) # beautiful tables

```

## The Equation

The equation for the multilevel model can be written in several ways: as multiple levels of equations; or as a single equation. The advantage of having multiple levels of equations is that these multiple equations make clear the multiple levels of the data, and thus conform to an initial understanding of how a multilevel model should be estimated. However, *results* from multiple levels of equations quickly become difficult to interpret, and thus, I will not spend a great deal of time on discussing empirical results of the two level formulation. Whether multiple levels of equations, or a single equation are employed, the numerical results are the same.

### Two Levels of Equations

I start with two levels of equations: Level 1 at the level of the individual; and Level 2 at the level of the country.

#### Level 1 (Individuals)

$$y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \beta_{2j} z_{ij} + e_{ij}$$ {#eq-MLM1}

#### Level 2 (Countries)

$$\beta_{0j} = \gamma_{00} +\gamma_{01} w_j + u_{0j}$$ {#eq-MLM2}

$$\beta_{1j} = \gamma_{10} + u_{1j}$$

$$\beta_{2j} = \gamma_{20}$$

$$\beta_{3j} = \gamma_{30}$$

Here $y_{ij}$ is the dependent variable, or outcome for the model. We note that the $ij$ subscripts indicate that this is outcome $y$ for individual $i$ in country $j$. Note that the outcome is at Level 1, or the level of individuals. $\beta_{0j}$ is a regression intercept, and the other $\beta$'s[^beta:] are regression slope parameters. $x_{ij}$ and $z_{ij}$ are independent variables and $t_{ij}$ is an independent variable indicating the time at which different data points are measured. I note that in this discussion I am *not* considering a model in which there are repeated observations on the same individuals, although the multilevel model is certainly extensible to such cases. $u_{0j}$ is a random intercept for the $\beta_{0j}$ term, and $u_{1j}$ is a random slope for the $\beta_{1j}$ term, indicating that we are modeling cross country variation in these parameters. The other $\beta$ terms are not modeled as having random country level variation, although this could certainly be a possibility in subsequent models. 

In this formulation of the multilevel model, each regression parameter $\beta$ in the level 1 equation is the outcome of an equation at Level 2. The parameters for the Level 2 equations are represented by $\gamma$'s. $w$ a Level 2 variable appears in the first Level 2 equation.

### One Level of Equations

By simply substituting the values of the Level 2 equations into the Level 1 equations, we obtain:

$$y_{ij} = \beta_0 + \beta_1 x_{ij} + \beta_2 z_{ij} + \beta_3 w_{j} + u_{0j} + u_{1j} \times x + e_{ij}$$ {#eq-MLM}

Here again $y_{ij}$ is the dependent variable, or outcome for the model. $\beta_0$ is a regression intercept, and the $\beta$'s are regression parameters. $x_{ij}$ and $z_{ij}$ are independent variables and $w$ is a Level 2 variable. 

While I purposely strive to keep the discussion of multilevel modeling ideas somewhat general in this document, I generally think of $y_{ij}$ as a desirable or "good" outcome. $x$ is often conceptualized as a protective factor, while $z$ is often conceptualized as a risk factor.

> Notice that in this *single equation* format all variables--no matter their *level*--appear in the same equation. 

In this formulation of the equation, the nature of the random effects is more clear, and merits discussion. Notice that we have included a *random intercept* $u_{0j}$ as well as a *random slope* $u_{1j} \times x$. The *random intercept*, $u_{0j}$, indicates that there is variation in the *intercept* of the country specific regression lines, as is true in @fig-data. The *random slope* term associated with $x$, $u_{1j} \times x$, indicates that there is also variation in the *slope* of the regression lines that is associated with $x$, in this case, the slope of parental warmth, as is also demonstrated in @fig-data. 

[^beta:]: Technically, all of these $\beta$'s could be written as $\beta_j$ since the multilevel model could be said to estimate a regression parameter for each group, in this case each country. One could even write $\beta_{jk}$ to represent the regression parameter for the $k^{th}$ independent variable the for the $j^{th}$ group or country. To keep matters simple, I simply write $\beta$ in most cases.

To make these ideas more concrete, I rewrite this equation in terms of the main substantive ideas of this document:

$$\text{outcome}_{ij} = \beta_0 + \beta_1 \text{parental warmth}_{ij} + \beta_2 \text{physical punishment}_{ij} + \beta_3 \text{HDI}_{j} \ + $$ {#eq-MLMsubstantive}

$$u_{0j} + u_{1j} \times \text{parental warmth} + e_{ij}$$ 

Drawing upon ideas from @sec-software, this single level equation can be easily represented in Stata syntax.

`mixed outcome warmth physical_punishment HDI || country: warmth`

## Estimating Standard Errors And p Values {#sec-pvalues}

### Introduction

If the data are grouped, nested, or clustered, then this aspect of the structure of the data needs to be accounted for. @Bland1994 describe a simulation in which grouped data are artificially generated according to the following procedure.

> "The data were generated from random numbers, and there is no relation between X and Y at all. Firstly, values of X and Y were generated for each 'subject,' then a further random number was added to make the individual observation." [@Bland1994]

```{r}

simulated_clustered_data <-
  read_dta("./simulate-and-analyze-multilevel-data/simulated_clustered_data.dta")

```

The graph below illustrates the process of simulating the data.

```{r}
#| fig-cap: "Simulated Clustered Data"
#| fig-height: 3

ggplot(simulated_clustered_data,
       aes(color = factor(group_numA),
           label = group_numA)) +
  geom_segment(aes(x = x_individualA, 
                y = y_individualA,
                xend = x_groupA,
                yend = y_groupA),
               show.legend = FALSE) +
  geom_point(aes(x = x_groupA, 
                 y = y_groupA),
             size = 10,
             alpha = 1,
             show.legend = TRUE) + 
  geom_text(aes(x = x_groupA,
                y = y_groupA),
            color = "white",
            show.legend = FALSE) +
  geom_label(aes(x = x_individualA, 
                y = y_individualA),
             show.legend = FALSE,
             size = 3) + 
  labs(title = "Illustrating the Process of Simulating the Data",
       x = "x",
       y = "y") +
  scale_color_viridis_d(name = "group") +
  theme_minimal() 

```

### Compare OLS and MLM

An analysis that is not aware of the grouped nature of the data will give biased results, will mis-estimate standard errors, and importantly, will often attribute statistical significance to some of the independent variables when this is not appropriate [@Raudenbush2002; @Bland1994]. 

In the example below, we compare a simple ordinary least squares analysis of the data with a multilevel model that accounts for the clustered nature of the data.

The Stata syntax that we use for each analysis is:

* OLS: `regress y x`
* Multilevel Model: `mixed y x || group:`

```{r, child=c('./simulate-and-analyze-multilevel-data/tableA.md')}
```

We see that in the ordinary least squares analysis, the independent variable is judged to have a statistically significant association with the dependent variable. The more appropriate multilevel model finds that in fact the independent variable $x$ is *not* associated with $y$. Thus, the multilevel model provides more accurate results than OLS in the presence of clustered data.

## Multilevel Structure {#sec-multilevelstructure}

Associations between two variables can be *very different* (or even *reversed*) depending upon whether or not the analysis is "aware" of the grouped, nested, or clustered nature of the data [@Gelman2007]. In the example presented here, the groups are countries, but could as easily be neighborhoods, communities, or schools.

For pedagogical purposes, I use an example with very few clusters, although it would be more appropriate to apply multilevel analysis to an example with many more clusters e.g. ($N_\text{clusters} >= 30$)

A model that is "aware" of the clustered nature of the data may provide very different--likely better--substantive conclusions than a model that is not aware of the clustered nature of the data. 

### Use Some Data Simulated For This Particular Example

```{r}

multilevelstructure <-
  read_dta("./simulate-and-analyze-multilevel-data/multilevelstructure.dta")

multilevelstructure$country <- factor(multilevelstructure$country)

```

### Graphs

#### A "Naive" Graph 

This "naive" graph is unaware of the grouped nature of the data. Notice that the overall regression line slopes downward, even though there is some suggestion that *within each group* the regression lines may slope upward.

```{r}
#| fig-cap: "A 'Naive' Graph"
#| fig-height: 3
#| label: fig-naive

p0 <- ggplot(multilevelstructure, 
             aes(x = x,
                 y = y)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "y as a function of x") +
  theme_minimal()

p0  # replay 

```

#### An "Aware" Graph 

This "aware" graph is aware of the grouped nature of the data. The graph is "aware" of the grouped or clustered nature of the data, and provides indication that the regression lines *when accounting for group* slope upward.

```{r}
#| fig-cap: "An 'Aware' Graph"
#| fig-height: 3
#| label: fig-aware

ggplot(multilevelstructure, 
             aes(x = x,
                 y = y,
                 color = country)) + # color is country
  geom_point() + 
  geom_smooth(method = "lm") + 
  scale_color_viridis_d() +
  theme_minimal()

```

### Regressions

#### A "Naive" OLS Analysis vs. An "Aware" MLM Analysis

The Stata syntax that we use for these analyses is: 

* OLS: `regress y x` 
* Multilevel Model: `mixed y x || country:`

The OLS model with only *x* as a covariate is not aware of the grouped structure of the data, and the coefficient for *x* in the OLS model reflects this. The coefficient for *x* in the OLS model is *negative*, and statistically significant.

The multilevel model is aware of the grouped structure of the data, and the coefficient for *x* in the multilevel model reflects this. The coefficient for *x* in the multilevel model is *positive*, and statistically significant.

```{r, child=c('./simulate-and-analyze-multilevel-data/tableB.md')}
```

### A Thought Experiment

When might a situation like this arise in practice? This is surprisingly difficult to think through. 

Imagine that *x* is a protective factor, or an intervention or treatment. Imagine that *y* is a desirable outcome, like improved mental health or psychological well being.

Now imagine that residents of countries provide more of the protective factor or more of the intervention in situations where there are lower levels of the desirable outcome. If one thinks about it, this is a very plausible situation. 

> A naive analysis that was unaware of the grouped nature of the data would therefore misconstrue the results, suggesting that the intervention was harmful, when it was in fact helpful.

```{r}
#| fig-cap: "A Heuristic Example"
#| fig-height: 3

p0 + 
  geom_point(aes(color = country)) + # points with country color
  geom_smooth(aes(color = country), # smoothers with country color
              method = "lm") + 
  labs(title = "Desirable Outcome as a Function of Intervention or Treatment",
       x = "intervention or treatment",
       y = "desirable outcome") +
  scale_color_viridis_d()

```

These data are constructed to provide this kind of extreme example, but it easy to see how multilevel thinking, and multilevel analysis may provide better answers than one would get if one ignored the grouped nature of the data.

## Regression With Simulated Multi-Country Data {#sec-regression}

After considering some of these broader issues, let's now examine the results of a multilevel regression with the simulated multicountry data. I will again imagine that the desirable outcome is an outcome such as improved psychological wellbeing. 

The Stata syntax that we use is:

`mixed outcome warmth physical_punishment HDI || country: warmth`

```{r, child=c('./simulate-and-analyze-multilevel-data/table1.md')}
```

The data suggest that parental warmth is positively associated with the desirable outcome, and that this result is statistically significant. Parental use of physical punishment is associated with statistically significant decreases in the desirable outcome. I note that there is some variation in the *constant* indicating that there is some variation in the initial or average levels of the desirable outcome--again improved psychological well-being--that is attributable to country.

There is also some variation in the slope associated with parental warmth that is attributable to country. Thus, while largely consistent, the relationship of parental warmth with child outcomes differs somewhat from country to country. 

`HDI`, the *Human Development Index*, our only country level, or Level 2, variable in this model is not associated with the outcome.

## Summary of Advantages Of The Multilevel Model

The discussion so far gives an idea of the advantages of the multilevel model for studying intrinsically multilevel data: children in classrooms or schools; individuals or families in neighborhoods; individuals or families in countries. These advantages can be summarized below:

1. Standard errors are estimated correctly as is statistical significance. This means that p values are correctly estimated accounting for the clustered or nested nature of the data. More colloquially, this most often means that we do not make the mistake of attributing statistical significance to a given risk or protective factor, when such a statistical significance is not warranted. Put even more straightforwardly correct estimation of standard errors and statistical significance prevents us from seeing results that are simply not present in the data, whether those concern risk factors or protective factors. 
2. Regression coefficients are estimated correctly accounting for the clustered or nested structure of the data. If one does not account for the clustered or nested structure of the data, regression slopes can be estimated as negative when they are more correctly estimated as positive, or as null, or conversely estimated as positive when there are more correctly seen as negative (or null). Again, to phrase things in a more colloquial fashion, this means that we do not judge something to be a risk factor when it is in fact a protective factor or a null effect; or a protective factor when it is in fact a risk factor, or a null effect.  
3. An increasing focus of statistical estimation is not to focus on particular regression parameters, but instead to predict outcomes for particular combinations of independent variables. Predictions from a multilevel model could be said to be best predictions in that groups are weighted by their precision, contributing to an estimate which makes better predictions than would a simple average. More colloquially, multilevel models allow us to predict outcomes better and more accurately than would be possible with simple or more naïve models.

## Predicted Values

According to **"Stein's Paradox"**, predictions from a multilevel model may be better than the mean.

**shrinkage**

## Variation

Above, in @sec-studyvariation, I have referred to multilevel models as the study of variation. Now that I have provided some discussion of the multilevel model, more statistical "unpacking" of ideas about variation is warranted. 

I provide again, for pedagogical purposes, the example substantive equation that I have been using in this document.

$$\text{outcome}_{ij} = \beta_0 + \beta_1 \text{parental warmth}_{ij} + \beta_2 \text{physical punishment}_{ij} + \beta_3 \text{time}_{ij} \ + $$ {#eq-MLMsubstantive2}

$$u_{0j} + u_{1j} \times \text{parental warmth} + e_{ij}$$ 

### Measured and Unmeasured Variation

The first thing to note about the equation is that it can be divided into measured and unmeasured variation. 

This is most easily seen if we introduce the idea of an unconditional model.

### Unconditional Model

The unconditional model is a model with no $x$'s or covariates [@Raudenbush2002]. 

$$\text{outcome}_{ij} = \beta_0 + u_{0j} + e_{ij}$$ {#eq-unconditional}

Here, $\text{outcome}_{ij}$ is a function of an intercept $\beta_0$, a country specific error term, $u_{0j}$, and an individual level error term $e_{ij}$.

Thus, all of the variation in $\text{outcome}_{ij}$ is--given the unconditional nature of our model--due to unmeasured variation at the country and individual level.

### Intra-Class Correlation Coefficient

I now introduce a measure known as the Intra-Class Correlation Coefficient, (ICC) that can be computed from this unconditional model [@Raudenbush2002].

$$\text{ICC} = \frac{var(u_{0j})}{var(u_{0j}) + var(e_{ij})}$$ {#eq-ICC}

Heuristically:

$$\text{ICC} = \frac{\text{group level variation}}{\text{group level variation} + \text{individual level variation}}$$ {#eq-ICCheuristic}

The ICC from the *unconditional* model (@eq-unconditional) is the most informative ICC as it represents the amount of variation in the dependent variable that could *potentially* be explained by the grouping variable.

As we add covariates, $x$'s, to the model the ICC will most often decrease.

```{r}
#| label: fig-variationsources
#| fig-cap: "Sources of Variation in a Multilevel Model"
#| fig-height: 6

tribble(
  ~beta0, ~beta1, ~u0, ~u1,
  5, 0, .5, .25) %>% 
  uncount(weights = 30) %>%
  mutate(country = factor(row_number())) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(intercept = beta0 + rnorm(1, 0, u0),
         slope = beta1 + rnorm(1, 0, u1)) %>%
  ggplot() +
  geom_abline(aes(intercept = intercept, 
                  slope = slope),
              color = "grey") +
  annotate(geom = "point", 
           x = 0, 
           y = 5, 
           color = "orange",
           alpha = .5,
           size = 17) +
  annotate("text", 
           x = 1.5, 
           y = 2.5, 
           label = "variation in y",
           color = "red") +
  annotate("segment", 
           x = 1, 
           xend = 0, 
           y = 3, 
           yend = 4.5,
           colour = "red", 
           size = 1, 
           arrow = arrow()) +
  annotate("segment", 
           x = 0, 
           xend = 10, 
           y = 1, 
           yend = 1,
           colour = "orange", 
           size = 1, 
           arrow = arrow(ends = "both")) +
  annotate("text",
           x = 5,
           y = .5,
           label = "variation in x",
           color = "red") +
  geom_labelabline(aes(intercept = 6, 
                       slope = .25, 
                       label = "variation in slope"),
                   size = 2,
                   text_only = TRUE,
                   textcolor = "red",
                   linecolor = "orange") +
  geom_labelabline(aes(intercept = 4, 
                       slope = -.25, 
                       label = "variation in slope"),
                   size = 2,
                   text_only = TRUE,
                   textcolor = "red",
                   linecolor = "orange") +
  labs(title = "Sources of Variation",
       subtitle = "In A Multilevel Model",
       x = "x",
       y = "y") +
  xlim(0, 10) +
  ylim(0, 10) +
  scale_color_viridis_d() +
  theme_minimal() 

```
 
### Variation In Intercepts or Outcomes

In @eq-MLMsubstantive2, $var(u_{0j})$ is the model estimated amount of variation in the *outcome*, $y_{ij}$.

In the regression in @sec-regression, there is discernible between country variation, but more of the variation is between individuals within the same country. Put another way, there is a moderate tendency for children in families in the same country to have similar outcomes, but two children in families in the same country may also have very different outcomes. Children from families in different countries may be as similar as children from families in the same country.

### Variation In Predictors

Equally important, I think, but much less frequently explored than variation in *outcomes*, is the possibility of variation in *predictors*. In the substantive example that we have employed so far, the *predictors* are different *parenting behaviors*, so considering variation in *predictors* allows us to consider variation in *parenting behaviors*, as well as variation in the *outcomes* of those behaviors.

We would estimate variation in behaviors in much the same way that we would estimate variation in outcomes, estimating an unconditional model, but substituting $x$ for $y$.

$$x_{ij} = \beta_0 + u_{0j} + e_{ij}$$ {#eq-unconditionalx}

Then, similarly, the variation in a predictor attributable to the clustered nature of the data--in this case the clustering of individuals in countries--is given by:

$$\text{ICC}_x = \frac{var(u_{0j})}{var(u_{0j}) + var(e_{ij})}$$ {#eq-ICCx}

### Variation in Slopes

Another possible type of variation to investigate is variation in the relationship of $x$ and $y$, which is represented in the multilevel model by examining variation in the $\beta$'s.

### Variation As An Outcome

Even less common is to examine *variation* itself as an outcome [@Burkner2018]. 

$$\sigma_{yij} = \beta_0 + \beta_1 x_1 + u_{0j} + e_{ij}$$ {#eq-distributional}

Here, the variation in the outcome, $\sigma_{yij}$, rather than the mean level of the outcome, $y_{ij}$, is the focus of interest. My notation for @eq-distributional draws upon @Burkner2018's notation, but is modified in order to be consistent with the rest of this document. 

Why might such models be of conceptual interest? Imagine for example, that the *variation* in psychological well-being is higher in countries with higher levels of poverty, or higher levels of income inequality. The use of such models as this, discussed in more detail by @Burkner2018, would allow us to explore such a question. 

Of note, while I do not explore in detail differences between Bayesian and frequentist approaches to multilevel modeling in this document, these models are likely to be only estimable with Bayesian software rather than with frequentist software [@Burkner2018]. 

### Maximal Models

Hypothetically, one might imagine that there could be group level unobserved factors which affect regression slopes—i.e. the relationship between a predictor x and outcome variable y—arguably, were one to ignore these unobserved factors in statistical estimation, they would show up either in an error term, or in the regression coefficients themselves. Were they to show up in the regression coefficients this would represent statistical bias and a substantive mis-estimation of important effects. thus, there is a conceptual argument for including as many random effects—i.e. random slopes—in a statistical model as possible.

Models with all possible random effects are termed *maximal models* [@Barr2013; @Frank2018]. Such models include a large number of random slopes, e.g. $u_1 \times x_1, u_2 \times x_2, u_3 \times x_3, ..., \text{etc.}$ even when some of those estimated slopes are close to 0. Such models may be more easily estimable when using Bayesian estimation [@Frank2018], a topic which I do not cover in detail in this document. 

It should be noted that @Matuschek2017 argue that such a *maximal* approach may lead to a loss of statistical power and further argue that one should adhere to "a random effect structure that is supported by the data." In contrast, @nalborczyk_batailler_loevenbruck_vilain_burkner_2019 argue that maximal models are supported under the Bayesian approach. @Oberauer2022 also argues for including multiple random slopes. @Schielzeth2009 make a similar argument from a frequentist perspective.

## Some Wrong (or Partially Wrong) Approaches {#sec-wrongapproaches}

When data are clustered--e.g. residents in neighborhoods, children in schools, families in countries--it is worth discussing the fact that we have several choices statistically as how to proceed, other than using a multilevel model. Given the discussion so far, we can see the advantages of a multilevel model over these other approaches: 

1. First, we could simply ignore the clustering, and treat the data as though it were composed of statistically independent individuals, i.e. statistically independent $e_i$. As we have discussed above, however, this approach has at least two disadvantages. First, as discussed in @sec-pvalues, this approach will mis-estimate standard errors, most often underestimating them, resulting in underestimated p values and false positives. Second, as discussed in @sec-multilevelstructure ignoring clustering runs the risk of estimating regression $\beta$'s that are not estimated with information about the multilevel structure of the data, with the possibility that $\beta$ coefficients may not only have incorrect statistical significance, but also incorrect magnitude, and even incorrect sign.
2. A second approach would be to *aggregate* the data to the level of the higher social unit, e.g. aggregating the data at the level of the neighborhood. Here we run into an idea similar to that discussed in @sec-multilevelstructure, the "ecological fallacy": the idea that group level and individual level relationships are necessarily the same [@FIREBAUGH20014023].
3. Lastly, we could adopt a statistical strategy of *clustering* the standard errors. Clustering the standard errors means that standard errors are corrected for the non-independence of the $e_i$ within clusters. Thus, *p* values are estimated correctly. However, clustering still does not account for the multilevel structure of the data (@sec-multilevelstructure), and thus when relationships between *x*'s and *y* at different levels of the data are very different, simply clustering the standard errors may not give correct estimates of the $\beta$'s.
