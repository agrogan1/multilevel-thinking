# Multilevel Logistic Regression {#sec-logistic}

> "We have peered into a new world and have seen that it is more mysterious and more complex than we had imagined." [@Rubin1997]

## Introduction

New forms of the multilevel model are required when we have outcomes that are not continuous. Let us imagine, for example, that we have a situation in which our continuous outcome is now categorized into two groups. For example, we might imagine that there is some sort of diagnostic cutoff. scores greater than this cutoff are assigned to one group (`1`), while scores lower than the cutoff are assigned to another group (`0`). 

No doubt this scenario is more likely when we consider an *undesirable* outcome like depression or anxiety. Higher levels of depression or anxiety might be greater than some diagnostic cutoff, meriting a score of `1`, *meets criteria for a diagnosis*, while scores lower than that diagnostic cutoff might receive a score of `0`, *does not meet diagnostic criteria*. However, in keeping with our characterization of the outcome in the simulated data employed in this book as desirable or beneficial, we may also imagine a situation in which `1` is assigned to sufficiently high levels of some desirable or beneficial outcome that exceed some threshold value, while scores below that value are assigned to be `0`. 

```{r}
#| fig-cap: "A Continuous Outcome Dichotomized"
#| fig-height: 3
#| label: fig-dichotomized
#| echo: false

library(ggplot2)

library(ggrepel)

N <- 100

outcome <-runif(N, 0, 100)

outcome_dichotomous <- ifelse(outcome > 50, 75, 25)

mydata <- data.frame(outcome, outcome_dichotomous)

ggplot(mydata) +
  geom_segment(aes(x = 0,
                   xend = 1,
                   y = outcome,
                   yend = outcome_dichotomous),
               color = "grey") +
  geom_point(aes(x = 0, y = outcome,
                 color = outcome)) +
  geom_point(aes(x = 1, 
                 y = outcome_dichotomous,
                 color = outcome_dichotomous),
             size = 7) +
  annotate("text", x = 1, 
           y = 25, 
           label = "0",
           color = "white") +
  annotate("text", x = 1, 
           y = 75, 
           label = "1",
           color = "white") +
  annotate("text", x = 1, 
           y = 50, 
           label = "dichotomous \noutcome",
           color = "black") +
  labs(title = "Continous Outcome Converted To Dichotomous Outcome",
       y = "continuous outcome") +
  xlim(0, 1.1) +
  theme_minimal() +
  theme(legend.position = "none")

```


Pedagogically, this new categorical outcome satisfies our need to have a dichotomous outcome to explain a new group of models--logistic regression models--that are suitable for such outcomes. It is sometimes considered to be a statistical rule of thumb that we should never dichotomize a continuous outcome. Certainly, it is true that when we dichotomize an outcome we lose a certain amount of information in the data about the variation, heterogeneity, or diversity in the outcome. Such a loss of information may reduce our ability to obtain statistically significant results. At the same time, certain numerical cutoffs may have important substantive meanings, clinical meanings, or policy meanings. For example I have already considered higher levels of anxiety or depression that may exceed clinically important cutoffs implying a diagnosis `1` versus `0` of depression or anxiety. Levels of income below a certain threshold--whether a country specific threshold or some globally relevant threshold--are considered to be in poverty[^povertyline], while individuals above that income threshold are considered to be not in poverty (`1` versus `0`). In the example considered in detail in this chapter, families with a child having a beneficial outcome at a certain level might be considered to have a child satisfying a certain minimal level of psychological well-being. 

In contrast, some outcomes are naturally dichotomous and do not arise from dichotomizing a continuous variable: born versus not born; married or partnered versus single; alive versus dead; entered a program; exited a program; conflict or protest occurred versus conflict or protest did not occur. 

## Two Strategies

I now consider two possible strategies from modeling dichotomous outcomes: the *linear multilevel model* that we have been considering so far in this book; and a *multilevel logistic regression* model designed specifically for dichotomous outcomes. The easiest way to consider and compare these models is through presentation.

```{r}
#| fig-cap: "Linear and Logistic Regression"
#| fig-height: 3
#| label: fig-linear-logistic
#| echo: false
#| message: false
#| warning: false

library(ggplot2)

library(patchwork)

N <- 100

x1 <- runif(N, -10, 10)

x2 <- runif(N, -10, 10)

beta1 <- 5

beta2 <- 1

z <- (beta1 * x1) + (beta2 * x2)

myprobability <- exp(z)/(1 + exp(z))

y <- ifelse(myprobability >= .5, 1, 0)

mydata <- data.frame(x1, myprobability, y)

p0 <- ggplot(mydata, 
             aes(x = x1, 
                 y = y)) +
  geom_point() +
  ylim(-.25, 1.25) +
  theme_minimal()
 
# p0 # replay

plinear <- p0 + 
  geom_smooth(method = "lm") +
  labs(title = "Linear Model")

plogistic <- p0 + 
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"),
              se = TRUE) +
  labs(title = "Logistic Model")

plinear + plogistic

```


In each model, the equivalent pattern of dots represents some dichotomous outcome (e.g. birth, death, satisfies diagnostic criterion) that becomes more likely as some independent variable increases in value. At higher levels of the independent variable, the outcome is almost exclusively `1`. at lower levels of the independent variable, the outcome is almost exclusively `0`. One could try to estimate this dichotomous outcome with a straight line, or linear model, as depicted in the left hand panel of figure XXX. This might be roughly plausible, and this procedure is termed a *linear probability model* which I will not discuss in more detail here. Suffice it to say that several problems emerge for a linear probability model: the model predicts values of the outcome greater than `1` and less than `0`; standard errors of the model are clearly not homoscedastic but change in value (heteroscedasticity); and lastly a linear model estimates a constant association between changes in *x* and changes in *y* when this is not appropriate (@fig-marginal-changes). 

```{r}
#| fig-cap: "Marginal Changes in *y* in Logistic Regression"
#| fig-height: 6
#| label: fig-marginal-changes
#| echo: false
#| message: false
#| warning: false

smootherdata <- ggplot_build(plogistic)$data[[2]]

plogistic + 
  geom_step(data = smootherdata,
                      aes(x = x,
                          y = y),
                      color = "red") +
  labs(subtitle = "Marginal Changes in y Are Different At Every Value of x")

```


## Probabilities and Odds

To being to develop a *multilevel logistic regression* model, it is useful to think about ideas of *probabilities* and *odds*. One possible definition of a probability is that it is the long run fraction of times that an event occurs in series of events **CITATION**. \index{probability}

For example, if there are 100 possible occasions for an event--for example, experiencing happiness--to occur, and happiness only occurs 10 times out of a possible 100, then we might say that the *probability* or *risk* of happiness is .10. If happiness occurs 90 times out of a possible 100 occurrences, then we would say that the *probability* or *risk* of happiness is .90. \index{risk}

In contrast, if happiness occurs 10 times out of a possible 100, then the *odds* of happiness occurring are `10/90 = 1/9 = .11`. Similarly if happiness occurs 90 times, then the *odds* of happiness occurring are `90/10 = 9/1 = 9` [@Viera2008]. \index{odds}

```{r}
#| tbl-cap: "Probabilities and Odds"
#| fig-height: 3
#| label: tbl-probabilities-odds
#| echo: false

`total occasions` <- rep(100, 10)

`event occurred` <- seq(10, 100, 10)

`event did not occur` <- `total occasions` - `event occurred`

risk <- `event occurred` / `total occasions`

odds <- `event occurred` / `event did not occur`

mydata <- data.frame(`total occasions`,
                     `event occurred`,
                     `event did not occur`,
                     risk,
                     odds,
                     check.names = FALSE)

pander::panderOptions('round',2)

pander::pander(mydata)

```

A few things are worth noting about risks and odds. First, one might think of the risks and the odds of simply different ways of estimating the chances that something will happen.

A classic diagram to illustrate these ideas appears in many sources. [e.g. @Viera2008]

$$\begin{matrix}
a & b \\
c & d 
\end{matrix}$$ #eq-event-matrix

* Then a *risk* is defined as $a/(a + b)$.
* $c/(c + d)$ is also a *risk*.
* $\frac{a/(a + b)}{c/(c + d)}$ is then a ratio of risks or *risk ratio*.

* $a/b$ is the *odds* of an event in the first row, while $c/d$ is the *odds* of an event in the second row. 
* $\frac{a/b}{c/d}$ is then a ratio of odds, or *odds ratio*.

## Logistic Regression

Logistic regression begins with the idea that we are predicting the odds of an event. A common way to write the odds is $\frac{p(y)}{1-p(y)}$, where $p(y)$ is the probability of an outcome of interest, while $1-p(y)$ is the probability that the outcome will not occur.  

We are estimating a function of these odds as a linear function of a set of predictors, $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$. Here $u_{0j}$ is the usual level 2 random intercept that I have been including all along in multilevel models (@eq-MLM). \index{logistic regression}

The expression $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$ is a linear function of the independent variables. In order make a linear prediction appropriate, we take the logarithm of the odds, which provides the equation for multilevel logistic regression. 

$$\ln\Big(\frac{p(y)}{1-p(y)}\Big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$$ {#eq-logistic-MLM}

Notice that while there is a level 2 error term, or random intercept $u_{0j}$, there is no level 1 error term ($e_{ij}$) as we saw in @eq-MLM. This is because the use of probabilities $p(y)$ encompasses the idea of error in the logistic regression model.

## Odds Ratios

In a logistic regression the dependent variable is the *log-odds* of the results (@eq-logistic-MLM). The log-odds is likely to be a less than intuitive metric. We can obtain more intuitive results if we *exponentiate* both sides of @eq-logistic-MLM (i.e. raise both sides to the power of $e$).

$$\frac{p(y)}{1-p(y)} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}}$$ #eq-logistic-MLM-exponentiated

Through simple algebraic rules around exponents, this then becomes:

$$\frac{p(y)}{1-p(y)}= e^{\beta_0} e^{\beta_1 x_1} e^{\beta_2 x_2} e^{u_{0j}}$$ #eq-logistic-MLM-exponentiated-multiplied

The quantity $e^{\beta}$ is termed an *odds ratio* and represents the association of a 1 unit change in the independent variable with the *odds* of the outcome.

## $\beta$ Coefficients and Odds Ratios

--------------------------------------------------
Substantively    $\beta$     OR
--------------   ---------- ----------
x is              $>0.0$        $>1.0$
associated
with an 
increase
in y

no                $0.0$         $1.0$
association
of x with
y

x is              $<0.0$        $<1.0$
associated
with a
decrease
in y
--------------------------------------------------

## Regression With Simulated Multi-Country Data {#sec-logistic-regression}

### Unconditional Model

As I have done earlier (@sec-unconditional), I first estimate an unconditional model:

```{r, child=c('./simulate-and-analyze-multilevel-data/tableL0.md')}
```

This unconditional model provides us with the variance of the random intercept ($\u_{0j}$) which I use below to calculate the intra-class correlation coefficient. 

### Intra-Class Correlation Coefficient

In logistic regression, as noted in @eq-logistic-MLM, there is no $e_{ij}$ error term. In order to calculate the intra-class correlation coefficient, we use the fact that a standard logistic distribution has a variance of $\frac{\pi^2}{3}$, or roughly 3.29 **CITATION**.

Hence, in a logistic regression, the equation for the intra-class correlation coefficient becomes: 

$$\text{ICC} = \frac{var(u_{0j})}{var(u_{0j}) + \frac{\pi^2}{3}}$$ {#eq-ICC-logistic}

Substituting in the appropriate quantities suggests that approximately 6.6% of the variation in the dichotomous outcome is potentially explainable by the clustering of observations in countries. 

### Conditional Model

I now estimate a *conditional* multilevel logistic regression model *with* independent variables.

```{r, child=c('./simulate-and-analyze-multilevel-data/tableL1.md')}
```

The model demonstrates a number of results that are substantively similar to those in @sec-conditional-model. Increased levels of parental warmth area associated with increased odds of being in the higher outcome category, while increased levels of physical punishment are associated with decreased odds of being in the higher outcome category. The identity category in these data is not associated with higher or lower odds of being in the higher outcome category. Participation in the intervention is associated with higher odds of being in the higher outcome category. The Human Development Index is not associated with different odds of being in the higher outcome category. 

## Predicted Probabilities

I have above discussed the ideas of estimating the association of independent variables with the odds of an outcome. However, while odds ratios are perhaps the most common way of reporting the results of logistic regressions in the empirical literature, there are at least several issues with odds ratios:

1. Odds ratios provide a convenient way of talking about the association of independent variables with categorical outcomes. At the same time, as shown in @tbl-probabilities-odds the odds will overstate the risk, especially as the odds become larger. 
2. Odds ratios provide one metric of association for the entire range of an independent variable $x$. In contrast, as shown in @fig-marginal-changes, marginal changes may be very small for very small values of the independent variable and very large values of the independent variable, while larger for intermediate ranges of the independent variable. Thus, odds ratios, while characterizing the overall logistic regression curve likely do not provide a finer grained picture of changes in the dependent variable at different values of the independent variable. 

For these reasons it is often convenient to calulcate predicted probabilities. \index{predicted probabilities}

To make the algebra easier, I first substitute $Z$ for $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$:

$$\ln\Big(\frac{p(y)}{1-p(y)}\Big) = Z$$
Then 

$$\Big(\frac{p(y)}{1-p(y)}\Big) = e^Z$$
$$p(y) = e^Z(1-p(y))$$
$$p(y) + e^Z(p(y)) = e^Z$$
$$p(y) = \frac{e^Z}{1+e^Z}$$
Finally, substituting back in the original expression for $Z$:

$$p(y) = \frac{e^Z}{1+e^Z}$$




[^povertyline]: For example the World Bank [@WorldBankPovertyLine] considers $2.15 per person per day to be a globally relevant indicator of extreme poverty.


