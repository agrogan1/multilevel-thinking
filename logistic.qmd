# Multilevel Logistic Regression {#sec-logistic}

> "We have peered into a new world and have seen that it is more mysterious and more complex than we had imagined." [@Rubin1997] \index{Vera Rubin}

## Introduction

New forms of the multilevel model are required when we have outcomes that are not continuous. Let us imagine, for example, that we have a situation in which our continuous outcome is now categorized into two groups. For example, we might imagine that there is some sort of diagnostic cutoff. Scores greater than this cutoff are assigned to one group (`1`), while scores lower than the cutoff are assigned to another group (`0`). 

No doubt this scenario is more likely when we consider an *undesirable* outcome like depression or anxiety. Higher levels of depression or anxiety might be greater than some diagnostic cutoff, meriting a score of `1`, *meets criteria for a diagnosis*, while scores lower than that diagnostic cutoff might receive a score of `0`, *does not meet diagnostic criteria*. However, in keeping with our characterization of the outcome in the simulated data employed in this book as desirable or beneficial, we may also imagine a situation in which `1` is assigned to sufficiently high levels of some desirable or beneficial outcome that exceed some threshold value, while scores below that value are assigned to be `0`. 

```{r}
#| fig-cap: "A Continuous Outcome Dichotomized"
#| fig-height: 3
#| label: fig-dichotomized
#| echo: false

library(ggplot2)

library(ggrepel)

N <- 100

outcome <-runif(N, 0, 100)

outcome_dichotomous <- ifelse(outcome > 50, 75, 25)

mydata <- data.frame(outcome, outcome_dichotomous)

ggplot(mydata) +
  geom_segment(aes(x = 0,
                   xend = 1,
                   y = outcome,
                   yend = outcome_dichotomous),
               color = "grey") +
  geom_point(aes(x = 0, y = outcome,
                 color = outcome)) +
  geom_point(aes(x = 1, 
                 y = outcome_dichotomous,
                 color = outcome_dichotomous),
             size = 7) +
  annotate("text", x = 1, 
           y = 25, 
           label = "0",
           color = "white") +
  annotate("text", x = 1, 
           y = 75, 
           label = "1",
           color = "white") +
  annotate("text", x = 1, 
           y = 50, 
           label = "dichotomous \noutcome",
           color = "black") +
  labs(title = "Continuous Outcome Converted To \nDichotomous Outcome",
       y = "continuous outcome") +
  xlim(0, 1.1) +
  theme_minimal() +
  theme(legend.position = "none")

```


Pedagogically, this new categorical outcome satisfies our need to have a dichotomous outcome to explain a new group of models--logistic regression models--that are suitable for such outcomes. It is sometimes considered to be a statistical rule of thumb that we should never dichotomize a continuous outcome. It is definitely true that when we dichotomize an outcome we lose a certain amount of information in the data about the variation, heterogeneity, or diversity in the outcome. Such a loss of information may reduce our ability to obtain statistically significant results. At the same time, certain numerical cutoffs may have important substantive meanings, clinical meanings, or policy meanings.  Additionally dichotomous outcomes may prove more intuitive for readers of our work to understand than are continuous outcomes. While there are sometimes quite strongly held statistical opinions that continuous variables should never be dichotomized [@Senn2005], my own personal belief is that sometimes variables dichotomized into categories are more easily communicated to practitioners, clinicians and community members, and in these cases, the importance of clear communication outweighs the statistical arguments against dichotomization. 

For example, I have already considered higher levels of anxiety or depression that may exceed clinically important cutoffs implying a diagnosis `1` versus `0` of depression or anxiety. Levels of income below a certain threshold--whether a country specific threshold or some globally relevant threshold--are considered to be in poverty[^povertyline], while individuals above that income threshold are considered to be not in poverty (`1` versus `0`). In the example considered in detail in this chapter, families with a child having a beneficial outcome at a certain level might be considered to have a child satisfying a certain minimal level of psychological well-being. 

In contrast, some outcomes are naturally dichotomous and do not arise from dichotomizing a continuous variable: born versus not born; married or partnered versus single; alive versus dead; entered a program; exited a program; conflict or protest occurred versus conflict or protest did not occur. 

## Two Strategies

I now consider two possible strategies from modeling dichotomous outcomes: the *linear multilevel model* that we have been considering so far in this book; and a *multilevel logistic regression* model designed specifically for dichotomous outcomes. The easiest way to consider and compare these models is to begin with visual presentation.

```{r}
#| fig-cap: "Linear and Logistic Regression"
#| fig-height: 3
#| label: fig-linear-logistic
#| echo: false
#| message: false
#| warning: false

library(ggplot2)

library(patchwork)

N <- 100

x1 <- runif(N, -10, 10)

x2 <- runif(N, -10, 10)

beta1 <- 5

beta2 <- 1

z <- (beta1 * x1) + (beta2 * x2)

myprobability <- exp(z)/(1 + exp(z))

y <- ifelse(myprobability >= .5, 1, 0)

mydata <- data.frame(x1, myprobability, y)

p0 <- ggplot(mydata, 
             aes(x = x1, 
                 y = y)) +
  geom_point() +
  ylim(-.25, 1.25) +
  labs(y = "probability of y") +
  theme_minimal()
 
# p0 # replay

plinear <- p0 + 
  geom_smooth(method = "lm") +
  labs(title = "Linear Model")

plogistic <- p0 + 
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"),
              se = TRUE) +
  labs(title = "Logistic Model")

plinear + plogistic

```


In each model, the equivalent pattern of dots represents some dichotomous outcome (e.g. birth, death, satisfies diagnostic criteria) that becomes more likely as some independent variable increases in value. At higher levels of the independent variable, the outcome is almost exclusively `1`. At lower levels of the independent variable, the outcome is almost exclusively `0`. In the middle range of the independent variable, there is a mixture of `1` and `0`. One could try to estimate this dichotomous outcome with a straight line, or linear model, as depicted in the left hand panel of @fig-linear-logistic. This might be roughly plausible, and this procedure is termed a *linear probability model* [@Agresti1997; @Long2014] which I will not discuss in more detail here. Suffice it to say that several problems emerge for a linear probability model: the model predicts values of the outcome greater than `1` and less than `0`; and a linear model estimates a constant association between changes in *x* and changes in *y* when this is not appropriate (@fig-marginal-changes). 

```{r}
#| fig-cap: "Marginal Changes in *y* in Logistic Regression"
#| fig-height: 5
#| label: fig-marginal-changes
#| echo: false
#| message: false
#| warning: false

smootherdata <- ggplot_build(plogistic)$data[[2]]

plogistic + 
  geom_step(data = smootherdata,
                      aes(x = x,
                          y = y),
                      color = "red") +
  labs(subtitle = "Marginal Changes in y Are Different At Every Value of x")

```


## Probabilities and Odds

To being to develop a *multilevel logistic regression* model, it is useful to think about ideas of *probabilities* and *odds*. One possible definition of a probability is that a probability is the long run fraction of times that an event occurs in series of events [@Freedman1991]. \index{probability}

For example, if there are 100 possible occasions to experience an event--such as to experience happiness--and happiness only occurs 10 times out of a possible 100, then we might say that the *probability* or *risk* of happiness is .10. If happiness occurs 90 times out of a possible 100 occurrences, then we would say that the *probability* or *risk* of happiness is .90. \index{risk}

In contrast, the *odds* of an event are equivalently the *count* of times that an event occurs weighed against the *count* of times that the event does not occur, or the *probability* that an event occurs weighed against the *probability* that an event does not occur [@Viera2008]. 

$$\text{odds} = \frac{\text{count of occurrences}}{\text{count of occurrences}} = \frac{p(\text{occurrence})}{p(\text{non-occurrence})} =$$ {#eq-odds-definition}

$$\frac{p(\text{occurrence})}{1-p(\text{occurrence})}$$

Thus, if happiness occurs 10 times out of a possible 100, then the *odds* of happiness occurring are `10/90 = 1/9 = .11`. Similarly if happiness occurs 90 times, then the *odds* of happiness occurring are `90/10 = 9/1 = 9` [@Viera2008]. \index{odds}

```{r}
#| tbl-cap: "Probabilities and Odds"
#| label: tbl-probabilities-odds
#| echo: false

`total occasions` <- rep(100, 10)

`event occurred` <- seq(10, 100, 10)

`event did not occur` <- `total occasions` - `event occurred`

risk <- `event occurred` / `total occasions`

odds <- `event occurred` / `event did not occur`

mydata <- data.frame(`total occasions`,
                     `event occurred`,
                     `event did not occur`,
                     risk,
                     odds,
                     check.names = FALSE)

pander::panderOptions('round',2)

pander::pander(mydata)

```

A few things are worth noting about risks and odds. First, one might think of the risks and the odds as simply different ways of thinking about and talking about the chances that something will happen.

A classic diagram to more fully explicate some of these ideas appears in many sources. [e.g. @Viera2008]

$$\begin{matrix}
a & b \\
c & d 
\end{matrix}$$ {#eq-event-matrix}

* Then a *risk* is defined as $a/(a + b)$.
* $c/(c + d)$ is also a *risk*.
* $\frac{a/(a + b)}{c/(c + d)}$ is then a ratio of risks or *risk ratio*.

* $a/b$ is the *odds* of an event in the first row, while $c/d$ is the *odds* of an event in the second row. 
* $\frac{a/b}{c/d}$ is then a ratio of odds, or *odds ratio*.

## Logistic Regression

Logistic regression begins with the idea that we are predicting the odds of an event. A common way to write the odds is $\frac{p(y)}{1-p(y)}$, where $p(y)$ is the probability of an outcome of interest, while $1-p(y)$ is the probability that the outcome did not occur.  

We are estimating a function of these odds as a linear function of a set of predictors, $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$. Here $u_{0j}$ is the usual level 2 random intercept that I have been including all along in multilevel models (@eq-MLM). \index{logistic regression}

The expression $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$ is a linear function of the independent variables. In order make a linear prediction appropriate, we take the logarithm of the odds, which provides the equation for multilevel logistic regression [@Stock2003; @RabeHesketh2022; @Long2014]. 

$$\ln\Big(\frac{p(y)}{1-p(y)}\Big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$$ {#eq-logistic-MLM}

Notice that while there is a level 2 error term, or random intercept $u_{0j}$, there is no level 1 error term ($e_{ij}$) as we saw in @eq-MLM. This is because the use of probabilities $p(y)$ encompasses the idea of error in the logistic regression model.

## Odds Ratios

In a logistic regression the dependent variable is the *log-odds* of the results (@eq-logistic-MLM). The log-odds is likely to be a less than intuitive metric. We can obtain more intuitive results if we *exponentiate* both sides of @eq-logistic-MLM (i.e. raise both sides to the power of $e$).

$$\frac{p(y)}{1-p(y)} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}}$$ {#eq-logistic-MLM-exponentiated}

Through simple algebraic rules around exponents, this then becomes:

$$\frac{p(y)}{1-p(y)}= e^{\beta_0} e^{\beta_1 x_1} e^{\beta_2 x_2} e^{u_{0j}}$$ {#eq-logistic-MLM-exponentiated-multiplied}

The quantity $e^{\beta}$ is termed an *odds ratio* and represents the association of a 1 unit change in the independent variable with the *odds* of the outcome.

## $\beta$ Coefficients and Odds Ratios

It is worth thinking about the relationship between $\beta$ coefficients in a logistic regression, and the associated odds ratio. Many times software only reports the odds ratios. There are simple rules connecting $\beta$ coefficients from regressions with the odds ratio.

--------------------------------------------------
Substantively    $\beta$     Odds Ratio
--------------   ---------- ----------
x is              $>0.0$        $>1.0$
associated
with an 
increase
in y

no                $0.0$         $1.0$
association
of x with
y

x is              $<0.0$        $<1.0$
associated
with a
decrease
in y
--------------------------------------------------

: Logistic Regression Coefficients and Odds Ratios {#tbl-coefficients-odds-ratios}

In turn, thinking about odds ratios of different magnitudes can become complicated. 

-------------------------------------------------------------
Odds Ratio for Group A   Interpretations
Compared to 
Group B
------------------------ ------------------------------------
.25                       The odds of the outcome for Group A
                          were 25% of the odds of Group B.
                          
                          The odds of the outcome for Group A
                          were 75% lower than for the Group B. 

1.0                       The odds of the outcome for Group A
                          were equivalent to (the same as)
                          the odds for Group B. 
                          
                          There was no difference between Group A
                          and Group B.
                          
1.5                       The odds of the outcome for Group A
                          were 1.5 times those for Group B. 

                          The odds of the outcome for Group A
                          were 50% higher than for Group B. 

2.0                       The odds of the outcome for Group A 
                          were 100% higher than those for the
                          Group B. (technically correct,
                          but at the very least confusing.)
                          
                          The odds of the outcome for Group A
                          were 2.0 (twice) those for Group B.

2.5                       The odds of the outcome for Group A
                          were 2.5 times those for the Group B. 

                          The odds of the outcome for Group A
                          were 150% higher than for Group B.
                          (technically correct,
                          but at the very least confusing.)
                          
-------------------------------------------------------------

: Describing Odds Ratios of Different Magnitudes {#tbl-odds-ratios-different-magnitudes}

## Regression With Simulated Multi-Country Data {#sec-logistic-regression}

### Unconditional Model

As I have done earlier (@sec-unconditional), I first estimate an unconditional model:

```{r, child=c('./simulate-and-analyze-multilevel-data/tableL0.md')}
```

This unconditional model provides us with the variance of the random intercept ($u_{0j}$) which I use below to calculate the intra-class correlation coefficient. 

### Intra-Class Correlation Coefficient

In logistic regression, as noted in @eq-logistic-MLM, there is no $e_{ij}$ error term. In order to calculate the intra-class correlation coefficient, we use the fact that a standard logistic distribution has a variance of $\frac{\pi^2}{3}$, or roughly 3.29 [@Long2014].

Hence, in a logistic regression, the equation for the intra-class correlation coefficient becomes: 

$$\text{ICC} = \frac{var(u_{0j})}{var(u_{0j}) + \frac{\pi^2}{3}}$$ {#eq-ICC-logistic}

Substituting in the appropriate quantities suggests that approximately 6.6% of the variation in the dichotomous outcome is potentially explainable by the clustering of observations in countries. 

### Conditional Model {#sec-conditional-logistic}

I now estimate a *conditional* multilevel logistic regression model *with* independent variables.

```{r, child=c('./simulate-and-analyze-multilevel-data/tableL1.md')}
```

The model demonstrates a number of results that are substantively similar to those in @sec-conditional-model. Here I use the rubric described in @tbl-odds-ratios-different-magnitudes to first describe each result more generally, and then more precisely[^odds-are-not-risks]. Increased levels of parental warmth are associated with increased odds of being in the higher outcome category. Specifically, the odds of the outcome increase 29% with each 1 unit increase in parental warmth. In contrast, increased levels of physical punishment are associated with decreased odds of being in the higher outcome category. Specifically, a 1 unit increase in the use of physical punishment is associated with a 25% decrease in the odds of the outcome. The identity category in these data is not associated with higher or lower odds of being in the higher outcome category. Participation in the intervention is associated with higher odds of being in the higher outcome category. Specifically, participation in the intervention is associated with a 19% increase in the odds of the outcome. The Human Development Index is not associated with different odds of being in the higher outcome category. 

[^odds-are-not-risks]: In describing these odds ratios, it is important to use the words odds, because, as demonstrated in @tbl-probabilities-odds, odds are not the same thing as risks or probabilities!

## Predicted Probabilities

I have above discussed the ideas of estimating the association of independent variables with the odds of an outcome. However, while odds ratios are perhaps the most common way of reporting the results of logistic regressions in the empirical literature, there are at least several issues with odds ratios:

1. Odds ratios provide a *convenient* way of talking about the association of independent variables with categorical outcomes. At the same time, as shown in @tbl-probabilities-odds the odds will overstate the risk, especially as the odds become larger. 
2. Odds ratios provide one metric of association for the entire range of an independent variable $x$. In contrast, as shown in @fig-marginal-changes-2, marginal changes in the outcome may be very small for very small values of the independent variable, and very large values of the independent variable, while marginal changes in the outcome are larger for intermediate ranges of the independent variable. Thus, odds ratios, while characterizing the overall logistic regression curve, do not provide a finer grained picture of changes in the dependent variable at different values of the independent variable. 

```{r}
#| fig-cap: "Constant Odds Ratio But Different Changes in Predicted Probability"
#| fig-height: 3
#| label: fig-marginal-changes-2
#| echo: false
#| message: false
#| warning: false

x <- seq(-10, 10)

y <- exp(x)/(1 + exp(x))

logisticdata <- data.frame(x,y)

library(ggplot2)

subtitle <- paste("A 1 Unit Change In the Independent Variable",
                  "\nIs Associated With Different Changes In",
                  "\nThe Predicted Probability Of y",
                  "\nAt Different Values Of x")

ggplot(logisticdata,
       aes(x = x,
           y = y)) +
  geom_point() +
  geom_line() +
  geom_step(color = "red") +
  theme_minimal() +
  labs(title = "For a Constant Odds Ratio",
       subtitle = subtitle,
       y = "probability of y")

```


For these reasons it is often convenient to calculate predicted probabilities at different values of the independent variables [CF @Long2014]. \index{predicted probabilities}

::: {.callout-tip}
### The Algebra May Be Helpful

Below, I provide algebra for calculating predicted probabilities. This algebra sometimes proves difficult for readers to follow but is provided for the sake of completeness. Note that software often automates the process of calculating these predicted probabilities.
:::

### Calculating Predicted Probabilities

To make the algebra easier, I first substitute $Z$ for $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u_{0j}$:

$$\ln\Big(\frac{p(y)}{1-p(y)}\Big) = Z$$
Then 

$$\Big(\frac{p(y)}{1-p(y)}\Big) = e^Z$$
$$p(y) = e^Z(1-p(y))$$
$$p(y) + e^Z(p(y)) = e^Z$$
$$p(y) = \frac{e^Z}{1+e^Z}$$
Finally, substituting back in the original expression for $Z$:

$$p(y) = \frac{e^Z}{1+e^Z}$$

### A Substantive Example

I now provide a substantive example. Recall the model estimated in @sec-conditional-logistic. This model suggests, as an example, that participation in the intervention is associated with an odds ratio of 1.192. Put another way, participation in the intervention is associated with a .19 or 19% increase in the odds of a beneficial outcome. Bearing in mind the ideas presented in @fig-marginal-changes-2, it is worth thinking about the values of the predicted probability of a beneficial outcome at different levels participation in the intervention.

According to the formulas presented above, which are again, usually automatically calculated by statistical software, I obtain @tbl-predicted-probabilities. 

```{r}
#| tbl-cap: "Predicted Probabilities Of A Beneficial Outcome"
#| label: tbl-predicted-probabilities
#| echo: false

library(haven)

library(dplyr)

mymargins <- read_dta("./simulate-and-analyze-multilevel-data/mymargins.dta")

mymargins <- mymargins %>% 
  select(c("_by1", "_margin")) %>% 
  rename(intervention = "_by1",
         `predicted probability` = "_margin") %>%
    mutate(`absolute change` = `predicted probability` - 
             lag(`predicted probability`),
           `relative change` = `predicted probability` / 
             lag(`predicted probability`))

pander::pander(mymargins)

```

Calculations in @tbl-predicted-probabilities suggest that when considered in terms of the predicted probability of a beneficial outcome, participation in the intervention is associated with a 3.2% *absolute* increase in the probability of a beneficial outcome, and 6.5% *relative* increase in the probability of a beneficial outcome. Whether one considers *absolute* or *relative* changes, these differences are far smaller than those suggested by the odds ratios. Bear in mind that odds ratios often overstate the changes in risk (@tbl-probabilities-odds).


[^povertyline]: For example the World Bank [@WorldBankPovertyLine] considers $2.15 per person per day to be a globally relevant indicator of extreme poverty.


